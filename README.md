# ğŸ›¸ GenAI Self-Build Workshop Series

**Demystifying Generative AI Through Hands-On Construction**

> "What I cannot create, I do not understand." â€” Richard Feynman

## What Is This?

A 6-part workshop series where participants build GenAI components **from scratch**. No magic, no black boxes â€” just Python and understanding.

Each workshop is **1 hour** (45 min teaching/demo + 15 min Q&A) and includes:
- Complete Python implementation **with debug mode**
- Interactive Streamlit demo
- Comprehensive test suite
- Presentation slides
- Cheatsheet and Q&A guide
- **ğŸ” "Under the Hood" documentation** revealing how it really works

## ğŸ¯ The Workshops

| # | Topic | Analogy | What You'll Build |
|---|-------|---------|-------------------|
| 1 | **Tokenization** | ğŸ›¸ Alien builds a codebook | Character, word, and BPE tokenizers |
| 2 | **Embeddings** | ğŸ—ºï¸ Creating a meaning map | Word vectors from co-occurrence |
| 3 | **Vector Database** | ğŸ“š Magic library shelves | In-memory vector store with similarity search |
| 4 | **Attention** | ğŸ‘€ Spotlight of focus | Self-attention with visualization |
| 5 | **Transformers** | ğŸ§  Complete alien brain | Mini decoder-only transformer |
| 6 | **RAG** | ğŸ” Search engine + brain | Complete retrieval-augmented generation |

## ğŸ“– The Story: Zara's Journey

The workshops are woven together by a **narrative thread** â€” the journey of **Zara**, a Zorathian scientist who lands on Earth and must learn to understand human language from scratch.

Each workshop is a chapter in her story:

| Chapter | Zara's Challenge | What She Builds |
|---------|-----------------|-----------------|
| ğŸ“• The Codebook | Can't read human text | A tokenizer to turn symbols â†’ numbers |
| ğŸ“— The Map | Numbers don't capture meaning | An embedding space where similar = nearby |
| ğŸ“˜ The Library | Too many vectors to search | A vector database with instant lookup |
| ğŸ“™ The Spotlight | Context keeps confusing her | An attention mechanism to focus |
| ğŸ““ The Brain | Needs to assemble everything | A complete transformer architecture |
| ğŸ“” The Search Engine | Her brain hallucinates | RAG â€” look things up before answering |

> *By the end, Zara has built every component of a modern AI system â€” and so have you.*

See **[ğŸ“– Story Guide](docs/STORY_GUIDE.md)** for the complete narrative with presenter scripts, transition lines, and emotional arc guidance.

## ğŸ“· Screenshots

<p align="center">
  <img src="docs/pictures/unified-demo.png" alt="Unified Demo" width="800"/>
</p>

<details>
<summary>ğŸ“¸ View All Workshop Screenshots</summary>

| Workshop | Screenshot |
|----------|------------|
| 1. Tokenization | ![Tokenization](docs/pictures/01-tokenization.png) |
| 2. Embeddings | ![Embeddings](docs/pictures/02-embeddings.png) |
| 3. Vector DB | ![Vector DB](docs/pictures/03-vector-db.png) |
| 4. Attention | ![Attention](docs/pictures/04-attention.png) |
| 5. Transformers | ![Transformers](docs/pictures/05-transformers.png) |
| 6. RAG | ![RAG](docs/pictures/06-rag.png) |

</details>

## ï¿½ğŸš€ Quick Start

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/genai-self-build.git
cd genai-self-build

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Mac/Linux

# Install dependencies
pip install -r requirements.txt
```

### Run the Unified Demo

```bash
streamlit run app.py
```

This launches an interactive demo that covers all 6 workshops with an end-to-end pipeline visualization.

### Run Individual Workshops

```bash
# Run any workshop's interactive demo
cd workshops/01-tokenization
streamlit run app.py

# Run tests
python test_tokenizer.py
```

## ğŸ“ Project Structure

```
genai-self-build/
â”œâ”€â”€ app.py                    # ğŸ® Unified demo (all workshops)
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # This file
â”‚
â”œâ”€â”€ workshops/
â”‚   â”œâ”€â”€ 01-tokenization/      # ğŸ”¤ Text to numbers
â”‚   â”‚   â”œâ”€â”€ tokenizer.py      # Implementation
â”‚   â”‚   â”œâ”€â”€ app.py            # Streamlit demo
â”‚   â”‚   â”œâ”€â”€ test_tokenizer.py # Test suite
â”‚   â”‚   â”œâ”€â”€ slides/           # Presentation
â”‚   â”‚   â”œâ”€â”€ cheatsheet.md     # Quick reference
â”‚   â”‚   â”œâ”€â”€ qna.md            # Q&A guide
â”‚   â”‚   â””â”€â”€ README.md         # Workshop docs
â”‚   â”‚
â”‚   â”œâ”€â”€ 02-embeddings/        # ğŸ—ºï¸ Meaning in vectors
â”‚   â”œâ”€â”€ 03-vector-databases/  # ğŸ“š Semantic search
â”‚   â”œâ”€â”€ 04-attention/         # ğŸ‘€ Focus mechanism
â”‚   â”œâ”€â”€ 05-transformers/      # ğŸ§  The architecture
â”‚   â””â”€â”€ 06-rag/               # ğŸ” Retrieval + generation
â”‚
â””â”€â”€ docs/
    â””â”€â”€ workshop-plan.md      # Curriculum guide
```

## ğŸ“‹ Workshop Contents

Each workshop folder contains:

| File | Description |
|------|-------------|
| `*.py` | Core implementation with detailed comments |
| `app.py` | Interactive Streamlit demo |
| `test_*.py` | Comprehensive test suite |
| `slides/slides.md` | Marp presentation |
| `cheatsheet.md` | 1-2 page quick reference |
| `qna.md` | Anticipated questions & answers |
| `README.md` | Workshop documentation |
| `requirements.txt` | Dependencies |

## ğŸ”— How It All Connects

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    The GenAI Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ğŸ“ Text  â†’  ğŸ”¤ Tokens  â†’  ğŸ“Š Embeddings  â†’  ğŸ” Search         â”‚
â”‚  (input)    (Workshop 1)   (Workshop 2)     (Workshop 3)       â”‚
â”‚                                                                 â”‚
â”‚                    â†“                                            â”‚
â”‚                                                                 â”‚
â”‚  ğŸ’¬ Answer  â†  ğŸ§  Transform  â†  ğŸ‘€ Attend  â†  ğŸ“„ Context       â”‚
â”‚  (output)     (Workshop 5)    (Workshop 4)   (Workshop 6)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Prerequisites

- **Python:** Basic knowledge (functions, loops, lists, dictionaries)
- **Command line:** Basic familiarity
- **AI/ML:** No experience required!

## ğŸ“š Dependencies

Core dependencies (minimal by design):
- `numpy>=1.21.0` â€” Numerical operations
- `streamlit>=1.28.0` â€” Interactive demos

## ğŸ¯ Philosophy

We build everything from scratch (or near-scratch) using NumPy:
- **No hidden library calls** â€” Every operation is visible
- **Simplified but real** â€” Same algorithms, smaller scale
- **Learn by doing** â€” Build it yourself to understand it

### ğŸ” Under the Hood Features

**NEW!** All implementations include debug mode for deep learning:

```python
from tokenizer import SimpleTokenizer

# Enable debug mode to see internals
tokenizer = SimpleTokenizer(strategy='word', debug=True)
tokenizer.train(corpus)  # Shows step-by-step training process

# Get detailed statistics
stats = tokenizer.get_stats()
print(f"Vocabulary covers {stats['unique_words']} words")
print(f"Compression ratio: {stats['compression_ratio']:.1f}x")
```

**What you'll see:**
- âœ… Step-by-step algorithm execution
- âœ… Intermediate data transformations
- âœ… Performance metrics and statistics
- âœ… Decision points and why they're made
- âœ… Comparison to production systems

**Try it:**
```bash
cd workshops/01-tokenization
python under_the_hood_demo.py  # Interactive demo with debug mode
```

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| **[ğŸ“– Story Guide](docs/STORY_GUIDE.md)** | Narrative framework with presenter scripts and transition lines |
| **[User Guide](docs/USER_GUIDE.md)** | Getting started, running demos, learning tips |
| **[Teacher Guide](docs/TEACHER_GUIDE.md)** | Facilitation tips, session timelines, handling Q&A |
| **[Workshop Plan](docs/workshop-plan.md)** | Complete curriculum and session details |
| **[ğŸ” Under the Hood](docs/UNDER_THE_HOOD.md)** | Deep dive into algorithms, comparisons to production systems |

## ğŸ“– Additional Resources

After completing the workshops:
- **"Attention Is All You Need"** â€” The original transformer paper
- **"The Illustrated Transformer"** â€” Jay Alammar's visual guide
- **Andrej Karpathy's YouTube** â€” "Let's build GPT from scratch"
- **Hugging Face Course** â€” Practical NLP with transformers

## ğŸ“„ License

MIT â€” Use freely for education and learning.

## ğŸ‘¤ Author

**Michael Kennedy**  
ğŸ“§ michael.kennedy@analog.com

---

<div align="center">

**ğŸ›¸ Demystifying AI, one concept at a time**

*Built to make AI understandable*

</div>
