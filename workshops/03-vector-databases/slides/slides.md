---
marp: true
theme: default
paginate: true
backgroundColor: #1a1a2e
color: #ffffff
---

<!-- _class: lead -->

# ğŸ“š Workshop 3: Vector Databases
## *The alien's library with magic shelves*

### GenAI Self-Build Series (3 of 6)

---

# ğŸ¯ Learning Objectives

By the end of this workshop, you will understand:

1. **Why** we need specialized databases for vectors
2. **Three indexing strategies**: Flat, LSH, and IVF
3. **Trade-offs** between speed, accuracy, and memory
4. **How** production vector databases work

---

# ğŸ“– Previously on GenAI Self-Build...

**Workshop 1: Tokenization** ğŸ›¸
- Alien learned to read symbols â†’ tokens
- Text becomes numbers

**Workshop 2: Embeddings** ğŸ—ºï¸
- Alien created a meaning map
- Similar words live close together

**Today's Question:**
> With MILLIONS of word vectors, how do we find similar ones FAST?

---

# ğŸ¤” The Problem

Imagine searching for books similar to "Introduction to AI":

| Library Size | Books to Check | Time |
|--------------|----------------|------|
| 100 books | 100 | Instant |
| 10,000 books | 10,000 | A few seconds |
| 1 million books | 1,000,000 | Minutes! |
| 1 billion books | 1,000,000,000 | Hours!! |

**Linear search doesn't scale!**

---

# ğŸ“š The Library Analogy

The alien's library has 1 MILLION books (vectors).

**Brute Force Approach:**
```
Walk to shelf 1, check book... no match
Walk to shelf 2, check book... no match
Walk to shelf 3, check book... no match
... (999,997 more times)
```

**Smart Librarian Approach:**
```
"You want AI books? That's Section C!"
Walk directly to Section C
Check only 1,000 books in that section
```

**1000x faster!**

---

# ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“š Vector Database                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚   ğŸ“„ Vectors   â†’   ğŸ“Š Index   â†’   ğŸ” Search   â†’   ğŸ¯     â”‚
â”‚   (stored)         (organized)     (query)        Results â”‚
â”‚                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Index Strategy Options:                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ğŸ“– Flat  â”‚    â”‚ğŸ² LSH   â”‚    â”‚ğŸ“Š IVF   â”‚             â”‚
â”‚   â”‚ Exact   â”‚    â”‚ Hashing â”‚    â”‚Clusters â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“– Strategy 1: Flat (Brute Force)

### The Simplest Approach

```python
def search(query, vectors):
    scores = []
    for vector in vectors:  # Check EVERY vector
        score = cosine_similarity(query, vector)
        scores.append(score)
    return top_k(scores)
```

| Pros | Cons |
|------|------|
| âœ… 100% accurate | âŒ O(n) - slow |
| âœ… Simple | âŒ Doesn't scale |
| âœ… No index needed | âŒ Memory intensive |

**Use when:** < 10,000 vectors

---

# ğŸ² Strategy 2: LSH (Locality Sensitive Hashing)

### The Magic Coin Analogy

Imagine a magic coin that:
- Lands **HEADS** for "happy" ğŸª™ â¡ï¸ H
- Lands **HEADS** for "joyful" ğŸª™ â¡ï¸ H
- Lands **TAILS** for "sad" ğŸª™ â¡ï¸ T

**Similar items get similar coin flips!**

With multiple coins: `happy â†’ HHTH`, `joyful â†’ HHTH`, `sad â†’ TTHT`

Now just look in the "HHTH" bucket!

---

# ğŸ² LSH: How It Works

```
Step 1: Create random "coins" (hyperplanes)
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
Step 2: For each vector, flip all coins
        "happy"  â†’ [1,0,1,1] â†’ bucket "1011"
        "joyful" â†’ [1,0,1,1] â†’ bucket "1011"  (same!)
        "sad"    â†’ [0,1,0,0] â†’ bucket "0100"
        
Step 3: At query time
        Query "glad" â†’ [1,0,1,1] â†’ bucket "1011"
        Only search bucket "1011"!
```

---

# ğŸ² LSH: Performance

| Pros | Cons |
|------|------|
| âœ… Very fast lookup | âŒ Approximate |
| âœ… Works for any dimension | âŒ Tuning required |
| âœ… Memory efficient | âŒ Hash collisions |

**Parameters:**
- `num_tables`: More tables = better recall, slower
- `num_bits`: More bits = smaller buckets, more precise

---

# ğŸ“Š Strategy 3: IVF (Inverted File Index)

### The Section System

```
Library Sections:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“š Section A      ğŸ“š Section B      ğŸ“š Section C â”‚
â”‚  (Romance)        (Sci-Fi)          (Technical)  â”‚
â”‚                                                  â”‚
â”‚  â— â— â—            â— â— â—             â— â— â— â—      â”‚
â”‚  â— â— â—            â— â— â—             â— â— â— â—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query: "Machine Learning textbook"
â†’ Go to Section C (Technical)
â†’ Search only those books
```

---

# ğŸ“Š IVF: How It Works

```
Step 1: TRAINING (k-means clustering)
        Divide 1M vectors into 100 clusters
        Each cluster has ~10,000 vectors
        
Step 2: INDEXING
        Assign each vector to its nearest cluster
        Store in inverted file structure
        
Step 3: SEARCH
        Find nearest cluster(s) to query
        Search only those clusters
        nprobe=1: Search 1 cluster (fast, ~90% recall)
        nprobe=5: Search 5 clusters (slower, ~99% recall)
```

---

# ğŸ“Š IVF: Performance

| Pros | Cons |
|------|------|
| âœ… Good speed/accuracy | âŒ Training needed |
| âœ… Tunable via nprobe | âŒ Approximate |
| âœ… Industry standard | âŒ Cluster updates tricky |

**Parameters:**
- `num_clusters`: More = faster search, more training
- `nprobe`: How many clusters to check (speed/accuracy)

---

# ğŸ†š Strategy Comparison

|  | Flat | LSH | IVF |
|--|------|-----|-----|
| **Build** | O(1) | O(n Ã— tables) | O(n Ã— k-means) |
| **Search** | O(n) | O(1) ~ O(n) | O(k Ã— cluster) |
| **Accuracy** | 100% | 80-95% | 90-99% |
| **Memory** | Low | Medium | Medium |
| **Best for** | < 10K | High-dim | 100K-10M |

---

# ğŸ¯ Live Demo Time!

Let's explore our vector database:

```bash
cd workshops/03-vector-databases
streamlit run app.py
```

We'll see:
1. ğŸ“Š How indexing affects search
2. ğŸƒ Speed vs accuracy trade-offs
3. ğŸ¨ Vector space visualization

---

# ğŸŒ Real-World Vector Databases

| Database | Primary Algorithm | Notable Users |
|----------|------------------|---------------|
| **Pinecone** | IVF + HNSW | OpenAI, Notion |
| **Weaviate** | HNSW | Various startups |
| **Milvus** | Multiple options | Enterprise AI |
| **FAISS** | All of above | Meta, Research |
| **Chroma** | HNSW | LangChain apps |

Most use **HNSW** (graph-based) - even faster than IVF!

---

# ğŸ”— Connection to RAG (Workshop 6)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  User Question                                       â”‚
â”‚       â”‚                                              â”‚
â”‚       â–¼                                              â”‚
â”‚  [Embedding Model] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚       â”‚                          â”‚                   â”‚
â”‚       â–¼                          â–¼                   â”‚
â”‚  Query Vector  â”€â”€â†’  ğŸ“š Vector DB  â”€â”€â†’  Context      â”‚
â”‚                         â–²                            â”‚
â”‚                         â”‚                            â”‚
â”‚                   Documents                          â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Vector DB is the retrieval engine for RAG!**

---

# ğŸ’¡ Key Insights

1. **Linear search doesn't scale**
   - 1M vectors Ã— 1M queries = 1 trillion operations

2. **Approximate is usually OK**
   - 95% recall is fine for most applications
   - Users won't notice the 5% difference

3. **It's all about trade-offs**
   - More accuracy = more time
   - More speed = less accuracy
   - Choose based on your use case

---

# ğŸ¯ Key Takeaways

1. **Vector databases** solve the "needle in a haystack" problem for AI

2. **Three main strategies:**
   - ğŸ“– Flat: Exact but slow
   - ğŸ² LSH: Hash similar items together
   - ğŸ“Š IVF: Organize into searchable clusters

3. **Production systems** use hybrid approaches (HNSW + IVF)

---

# â¡ï¸ Next Workshop: Attention ğŸ‘€

Now we know:
- âœ… How to tokenize text
- âœ… How to embed tokens into vectors
- âœ… How to find similar vectors fast

**Next question:**
> How does the model know what to focus on?

In "The cat sat on the mat", why does "sat" relate more to "cat" than "mat"?

**That's the attention mechanism!**

---

<!-- _class: lead -->

# ğŸ™‹ Q&A Time!

## Questions?

---

# ğŸ“š Resources

- **Code**: `workshops/03-vector-databases/`
- **Cheatsheet**: `cheatsheet.md`
- **Q&A**: `qna.md`
- **FAISS Tutorial**: [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)

### Workshop 3 of 6 | GenAI Self-Build Series
*The alien's library with magic shelves - finding similar things fast!* ğŸ“š
