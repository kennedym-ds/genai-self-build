# üîç Workshop 6: RAG - Q&A

## Conceptual Questions

### Q: Why do we need RAG if LLMs already have so much knowledge?

**The Search Engine Analogy:** Imagine our alien studied every book in the universe during school (training), but that was 6 months ago. Now someone asks about yesterday's news - the alien has no idea! Even worse, instead of saying "I don't know," the alien might confidently make something up.

**Technical Answer:** LLMs have three fundamental limitations:
1. **Knowledge Cutoff**: Training data has a fixed date
2. **Hallucination**: Models generate plausible but false information
3. **No Citations**: Can't verify where information came from

RAG solves all three by treating the LLM as a *reasoning engine* rather than a *knowledge store*. The knowledge lives in a searchable database, and the LLM's job is just to synthesize and present it.

---

### Q: How is RAG different from fine-tuning?

**The Analogy:** 
- **Fine-tuning** = Sending the alien back to school to learn new things permanently
- **RAG** = Giving the alien a library card and teaching them to look things up

**Technical Answer:**

| Aspect | Fine-tuning | RAG |
|--------|-------------|-----|
| Cost | High (retraining) | Low (just indexing) |
| Latency | Fast inference | Slower (retrieval step) |
| Updates | Need retraining | Just update documents |
| Attributable | No | Yes (sources) |
| Best for | Style/behavior changes | Knowledge/facts |

In practice, many systems use both: fine-tune for behavior, RAG for knowledge.

---

### Q: What makes a "good" embedding for RAG?

**The Map Analogy:** If two restaurants that serve Italian food end up on opposite sides of your meaning map, you won't find them together when searching. A good embedding puts semantically similar things close together.

**Technical Answer:** Good RAG embeddings should:
1. **Capture semantic similarity** (not just keyword matching)
2. **Handle different phrasings** ("ML" ‚âà "machine learning")
3. **Work well for your domain** (code embeddings ‚â† legal embeddings)
4. **Be computationally efficient** at scale

Popular choices: OpenAI ada-002, Cohere embed, BAAI/bge, sentence-transformers.

---

### Q: How do you choose the right value for top-K?

**The Library Analogy:** If you're researching a topic, how many books do you grab from the shelf? Too few and you might miss important information. Too many and you're overwhelmed with irrelevant details.

**Technical Answer:**
- **Too low (K=1)**: Might miss relevant context, fragile if top result is wrong
- **Too high (K=10+)**: Noise dilutes signal, exceeds context window
- **Sweet spot (K=3-5)**: Usually sufficient, balances recall and precision

Also consider:
- **Score thresholds**: Only include docs above similarity threshold
- **Context window limits**: More docs = less room for generation
- **Document length**: Short chunks allow more documents

---

## Technical Questions

### Q: How does the vector search actually work?

**Technical Answer:** Our implementation uses cosine similarity:

```python
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

For each query:
1. Embed the query to get a vector
2. Compare against all document vectors
3. Return top-K highest similarity scores

**At Scale:** Real systems use approximate nearest neighbor (ANN) algorithms:
- FAISS (Facebook)
- Annoy (Spotify)
- HNSW (hierarchical navigable small world)

These trade exactness for speed, finding 95%+ of true neighbors in milliseconds.

---

### Q: What goes into the augmented prompt?

**Our Implementation:**
```
Answer the following question based on the provided context.
If the context doesn't contain enough information, say so.

Context:
[Source: docs/python.md]
Python is a programming language...

[Source: docs/ml.md]
Machine learning is...

Question: What is Python used for?

Answer:
```

**Key Decisions:**
1. **Instruction**: Tell the model to use the context
2. **Context format**: Include source attribution
3. **Context order**: Most relevant first (usually)
4. **Question placement**: After context (model sees context first)

---

### Q: What if retrieved documents contradict each other?

**The Library Analogy:** You found two books - one says "Coffee is healthy" and one says "Coffee is harmful." What does the alien do?

**Technical Approaches:**
1. **Trust scores**: Weight by source reliability
2. **Recency**: Prefer newer documents
3. **Consensus**: Look for majority agreement
4. **Transparency**: Show both viewpoints with sources
5. **Let the LLM decide**: Include instruction to handle contradictions

Example prompt addition:
```
If sources contradict each other, acknowledge the disagreement 
and present both viewpoints with their sources.
```

---

### Q: How do you handle multi-turn conversations with RAG?

**Technical Answer:** Each turn can trigger new retrieval:

```python
# Option 1: Retrieve on every turn
def chat(self, message, history=[]):
    # Include history in retrieval query
    full_query = " ".join([m['content'] for m in history[-3:]] + [message])
    docs = self.retrieve(full_query)
    ...

# Option 2: Retrieve only when needed
def chat(self, message, history=[]):
    if needs_retrieval(message):  # Classify the message
        docs = self.retrieve(message)
    else:
        docs = history[-1].get('docs', [])  # Reuse previous
```

---

## Comparison Questions

### Q: RAG vs Long Context Windows - which is better?

**The Analogy:** Should the alien carry all books in a massive backpack, or just carry a library card?

| Approach | Pros | Cons |
|----------|------|------|
| Long Context | Simpler, no retrieval | Expensive, slow, still limited |
| RAG | Scalable, efficient | Complex, retrieval errors |
| Hybrid | Best of both | Most complex |

**Answer:** For < 100 documents, long context might work. For 1000+, RAG is essential.

---

### Q: Dense retrieval vs Keyword search?

**Dense (Embedding-based):**
- ‚úÖ Understands synonyms and paraphrases
- ‚úÖ Works across languages
- ‚ùå Can miss exact matches
- ‚ùå More compute

**Sparse (BM25/TF-IDF):**
- ‚úÖ Great for exact keyword matches
- ‚úÖ Fast and interpretable
- ‚ùå Misses semantic similarity
- ‚ùå Language-specific

**Best Practice:** Use both! Hybrid retrieval combines dense and sparse scores.

---

## Real-World Questions

### Q: How does ChatGPT do retrieval?

**Answer:** ChatGPT uses several retrieval mechanisms:
1. **Web Browsing**: Bing search integration for current info
2. **Code Interpreter**: Can search uploaded files
3. **GPTs/Plugins**: Custom knowledge bases per app
4. **Memory**: User-specific long-term storage

When you upload a PDF, ChatGPT chunks it, embeds chunks, and retrieves relevant sections per query - exactly like our RAG implementation!

---

### Q: What about security? Can RAG leak private data?

**The Concern:** If sensitive documents are in the knowledge base, could someone craft a query to extract them?

**Mitigations:**
1. **Access control**: Only retrieve docs user is authorized to see
2. **Query filtering**: Block certain types of queries
3. **Output filtering**: Scan responses for sensitive patterns
4. **Chunking strategy**: Avoid chunks that contain complete sensitive records

This is an active research area called "RAG security" or "retrieval security."

---

### Q: How do production RAG systems handle updates?

**Answer:** Document lifecycle management:
1. **Incremental indexing**: Add new docs without rebuilding
2. **Document versioning**: Track which version was retrieved
3. **TTL (Time to Live)**: Automatically expire old documents
4. **Re-ranking**: Boost recent documents in results
5. **Feedback loops**: Learn from user corrections

---

## üéì Key Takeaways

1. **RAG = Retrieval + Augmented Generation**
   - Search for relevant docs, add to prompt, generate answer

2. **Solves three LLM problems**
   - Hallucination, staleness, lack of citations

3. **Quality depends on**
   - Embedding quality, chunk size, retrieval accuracy

4. **This is how modern AI systems work**
   - Perplexity, ChatGPT with files, enterprise AI

5. **RAG combines ALL workshop concepts**
   - Tokenization ‚Üí Embeddings ‚Üí Vector DB ‚Üí Attention ‚Üí Transformers ‚Üí RAG
